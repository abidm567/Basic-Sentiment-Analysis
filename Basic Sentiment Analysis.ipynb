{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d4b72d",
   "metadata": {},
   "source": [
    "#  Basic Sentiment Analysis using Naive Bayes classifier on Movie Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c2668b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/41.5 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 666.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abidm\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.7/269.7 kB 17.3 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.7.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\abidm\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\abidm\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\abidm\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749d6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1142a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\abidm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37fe446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the'] [10113]\n",
      "['rock'] [34]\n",
      "['is'] [3559]\n",
      "['destined'] [8]\n",
      "['to'] [4234]\n",
      "['be'] [939]\n",
      "['21st'] [6]\n",
      "['century'] [18]\n",
      "[\"'s\"] [3537]\n",
      "['new'] [206]\n"
     ]
    }
   ],
   "source": [
    "short_pos = open(\"short_reviews/positive.txt\",encoding=\"latin-1\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",encoding=\"latin-1\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "#get all lines from positive as well as negative docs\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append( (r, \"pos\") )\n",
    "\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append( (r, \"neg\") )\n",
    "\n",
    "#get all words\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "for i in np.arange(10):\n",
    "    print(list(all_words.keys())[i:i+1], list(all_words.values())[i:i+1])\n",
    "       \n",
    "#chooose to use first 5000 words as features for our purpose.\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "word_features[:10]\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4179f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10664"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b560ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:5332]\n",
    "testing_set =  featuresets[5332:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1023ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 72.03675918979745\n",
      "Most Informative Features\n",
      "                   tries = True              neg : pos    =     15.3 : 1.0\n",
      "                provides = True              pos : neg    =     11.9 : 1.0\n",
      "                    warm = True              pos : neg    =     11.9 : 1.0\n",
      "              unexpected = True              pos : neg    =     11.2 : 1.0\n",
      "                   bland = True              neg : pos    =     10.1 : 1.0\n",
      "             pretentious = True              neg : pos    =     10.1 : 1.0\n",
      "                captures = True              pos : neg    =      9.9 : 1.0\n",
      "               inventive = True              pos : neg    =      9.9 : 1.0\n",
      "                powerful = True              pos : neg    =      9.6 : 1.0\n",
      "                  school = True              neg : pos    =      8.8 : 1.0\n",
      "                touching = True              pos : neg    =      8.8 : 1.0\n",
      "                  moving = True              pos : neg    =      8.7 : 1.0\n",
      "                 amazing = True              pos : neg    =      8.5 : 1.0\n",
      "                   quiet = True              pos : neg    =      8.5 : 1.0\n",
      "                  thanks = True              pos : neg    =      8.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777d7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    pred=classifier.classify(feats)\n",
    "    return pred, classifier.prob_classify(feats).prob(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23736aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 0.943961573059633)\n"
     ]
    }
   ],
   "source": [
    "print(sentiment(\"This movie was good and awesome. The acting was great, plot was wonderful, and amazing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb6597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 0.9986464663726083)\n"
     ]
    }
   ],
   "source": [
    "print(sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
